{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ToxicityFlair LIMPIO",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deurru/AISaturdaysProject/blob/master/ToxicityFlair_LIMPIO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYV12C2i0AM-",
        "colab_type": "text"
      },
      "source": [
        "# Toxicity Classifier using LSTM and Zalando´s Flair"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkUJmYX1y-YC",
        "colab_type": "text"
      },
      "source": [
        "### 01 - Basic start operations:\n",
        "Mount the GDrive structure containing the datasets, import and install all libraries we are going to use to work on data,  check if hardware accelerator is activated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSggPpVIE3F-",
        "colab_type": "code",
        "outputId": "4fd5367d-41db-41be-891c-1684189e9ac1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "# Mount Google Drive (On every new session)\n",
        "\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive/\")\n",
        "\n",
        "os.chdir(\"drive/My Drive\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw1TPR3JoEvF",
        "colab_type": "code",
        "outputId": "ebe743f1-5939-495c-c0b7-7547e5da5fe5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1231
        }
      },
      "source": [
        "# installs flair library\n",
        "!pip install flair"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flair\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/54/76374f9a448ca765446502e7f2bb53c976e9c055102290fe6f8b0b038b37/flair-0.4.1.tar.gz (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.1.0)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.28.1)\n",
            "Collecting segtok>=1.5.7 (from flair)\n",
            "  Downloading https://files.pythonhosted.org/packages/1d/59/6ed78856ab99d2da04084b59e7da797972baa0efecb71546b16d48e49d9b/segtok-1.5.7.tar.gz\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.0.3)\n",
            "Collecting mpld3>=0.3 (from flair)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 13.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from flair) (0.0)\n",
            "Collecting sqlitedict>=1.6.0 (from flair)\n",
            "  Downloading https://files.pythonhosted.org/packages/0f/1c/c757b93147a219cf1e25cef7e1ad9b595b7f802159493c45ce116521caff/sqlitedict-1.6.0.tar.gz\n",
            "Collecting deprecated>=1.2.4 (from flair)\n",
            "  Downloading https://files.pythonhosted.org/packages/9f/7a/003fa432f1e45625626549726c2fbb7a29baa764e9d1fdb2323a5d779f8a/Deprecated-1.2.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n",
            "Collecting pytorch-pretrained-bert>=0.6.1 (from flair)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 45.8MB/s \n",
            "\u001b[?25hCollecting bpemb>=0.2.9 (from flair)\n",
            "  Downloading https://files.pythonhosted.org/packages/57/90/8760eaa97c5a2f676f3f350fd43e79f8d9e4f9c42362c62f733e81e37d33/bpemb-0.2.12-py3-none-any.whl\n",
            "Requirement already satisfied: regex==2018.1.10 in /usr/local/lib/python3.6/dist-packages (from flair) (2018.1.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->flair) (1.16.3)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.8.3)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.2.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->flair) (0.20.3)\n",
            "Requirement already satisfied: wrapt<2,>=1 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.10.11)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (3.8.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.1->flair) (1.9.140)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.1->flair) (2.21.0)\n",
            "Collecting sentencepiece (from bpemb>=0.2.9->flair)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/95/7f357995d5eb1131aa2092096dca14a6fc1b1d2860bd99c22a612e1d1019/sentencepiece-0.1.82-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 35.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair) (2.49.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.2.3->flair) (41.0.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert>=0.6.1->flair) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert>=0.6.1->flair) (0.2.0)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.140 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert>=0.6.1->flair) (1.12.140)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert>=0.6.1->flair) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert>=0.6.1->flair) (1.24.2)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert>=0.6.1->flair) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert>=0.6.1->flair) (3.0.4)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.140->boto3->pytorch-pretrained-bert>=0.6.1->flair) (0.14)\n",
            "Building wheels for collected packages: flair, segtok, mpld3, sqlitedict\n",
            "  Building wheel for flair (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/55/6b/c12cf58209b8346f653a04f37dd8f607ab0e85a26238a23420\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/ee/a8/6112173f1386d33eebedb3f73429cfa41a4c3084556bcee254\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/57/d3/907c3ee02d35e66f674ad0106e61f06eeeb98f6ee66a6cc3fe\n",
            "Successfully built flair segtok mpld3 sqlitedict\n",
            "Installing collected packages: segtok, mpld3, sqlitedict, deprecated, pytorch-pretrained-bert, sentencepiece, bpemb, flair\n",
            "Successfully installed bpemb-0.2.12 deprecated-1.2.5 flair-0.4.1 mpld3-0.3 pytorch-pretrained-bert-0.6.2 segtok-1.5.7 sentencepiece-0.1.82 sqlitedict-1.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVfpXrQ790ln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Main imports here\n",
        "import torch\n",
        "import flair\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feoi1vuym-5b",
        "colab_type": "code",
        "outputId": "c798789c-055c-4c03-f440-622d22337784",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# version checks (not mandatory)\n",
        "print(torch.__version__)\n",
        "print(flair.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.1.0\n",
            "0.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CYbSHuZnGyu",
        "colab_type": "code",
        "outputId": "e0d9319e-689f-43fa-bd39-ef986185c73c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# checking for cuda (if false = GPU not activated. Go to Runtime > Change Runtime Type > Hardware Accelerator > Select \"GPU\")\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.version.cuda)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "10.0.130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "calB87lkAA43",
        "colab_type": "code",
        "outputId": "a669d615-1e04-4576-8a98-ee0005f1fc59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# directory & dataset unzipping statements. Execute once, then comment.\n",
        "!pwd # Checks working directory\n",
        "# !unzip Datasets/jigsaw_unintended_bias_in_toxicity_classification.zip -d Datasets"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwGmGu-s0TbU",
        "colab_type": "text"
      },
      "source": [
        "### 02 - Exploring the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oa84FyyZJ7Fe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define path to source file and load data into pandas dataframe\n",
        "train_path = \"./Datasets/train.csv\"\n",
        "test_path = \"./Datasets/test.csv\"\n",
        "trainset = pd.read_csv(train_path)\n",
        "testset = pd.read_csv(test_path)\n",
        "df = pd.concat([trainset,testset], sort=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTaOdx4FMwiN",
        "colab_type": "code",
        "outputId": "33be93d5-90b2-4d75-e2c5-a7dea644ecc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1192
        }
      },
      "source": [
        "# check shape, column names and basic stats of attributes of dataset\n",
        "print(trainset.shape)\n",
        "print(trainset.columns)\n",
        "print(trainset.describe())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1804874, 45)\n",
            "Index(['id', 'target', 'comment_text', 'severe_toxicity', 'obscene',\n",
            "       'identity_attack', 'insult', 'threat', 'asian', 'atheist', 'bisexual',\n",
            "       'black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu',\n",
            "       'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability',\n",
            "       'jewish', 'latino', 'male', 'muslim', 'other_disability',\n",
            "       'other_gender', 'other_race_or_ethnicity', 'other_religion',\n",
            "       'other_sexual_orientation', 'physical_disability',\n",
            "       'psychiatric_or_mental_illness', 'transgender', 'white', 'created_date',\n",
            "       'publication_id', 'parent_id', 'article_id', 'rating', 'funny', 'wow',\n",
            "       'sad', 'likes', 'disagree', 'sexual_explicit',\n",
            "       'identity_annotator_count', 'toxicity_annotator_count'],\n",
            "      dtype='object')\n",
            "                 id        target  severe_toxicity       obscene  \\\n",
            "count  1.804874e+06  1.804874e+06     1.804874e+06  1.804874e+06   \n",
            "mean   3.738434e+06  1.030173e-01     4.582099e-03  1.387721e-02   \n",
            "std    2.445187e+06  1.970757e-01     2.286128e-02  6.460419e-02   \n",
            "min    5.984800e+04  0.000000e+00     0.000000e+00  0.000000e+00   \n",
            "25%    7.969752e+05  0.000000e+00     0.000000e+00  0.000000e+00   \n",
            "50%    5.223774e+06  0.000000e+00     0.000000e+00  0.000000e+00   \n",
            "75%    5.769854e+06  1.666667e-01     0.000000e+00  0.000000e+00   \n",
            "max    6.334010e+06  1.000000e+00     1.000000e+00  1.000000e+00   \n",
            "\n",
            "       identity_attack        insult        threat          asian  \\\n",
            "count     1.804874e+06  1.804874e+06  1.804874e+06  405130.000000   \n",
            "mean      2.263571e-02  8.115273e-02  9.311271e-03       0.011964   \n",
            "std       7.873156e-02  1.760657e-01  4.942218e-02       0.087166   \n",
            "min       0.000000e+00  0.000000e+00  0.000000e+00       0.000000   \n",
            "25%       0.000000e+00  0.000000e+00  0.000000e+00       0.000000   \n",
            "50%       0.000000e+00  0.000000e+00  0.000000e+00       0.000000   \n",
            "75%       0.000000e+00  9.090909e-02  0.000000e+00       0.000000   \n",
            "max       1.000000e+00  1.000000e+00  1.000000e+00       1.000000   \n",
            "\n",
            "             atheist       bisexual  ...     parent_id    article_id  \\\n",
            "count  405130.000000  405130.000000  ...  1.026228e+06  1.804874e+06   \n",
            "mean        0.003205       0.001884  ...  3.722687e+06  2.813597e+05   \n",
            "std         0.050193       0.026077  ...  2.450261e+06  1.039293e+05   \n",
            "min         0.000000       0.000000  ...  6.100600e+04  2.006000e+03   \n",
            "25%         0.000000       0.000000  ...  7.960188e+05  1.601200e+05   \n",
            "50%         0.000000       0.000000  ...  5.222993e+06  3.321260e+05   \n",
            "75%         0.000000       0.000000  ...  5.775758e+06  3.662370e+05   \n",
            "max         1.000000       1.000000  ...  6.333965e+06  3.995410e+05   \n",
            "\n",
            "              funny           wow           sad         likes      disagree  \\\n",
            "count  1.804874e+06  1.804874e+06  1.804874e+06  1.804874e+06  1.804874e+06   \n",
            "mean   2.779269e-01  4.420696e-02  1.091173e-01  2.446167e+00  5.843688e-01   \n",
            "std    1.055313e+00  2.449359e-01  4.555363e-01  4.727924e+00  1.866589e+00   \n",
            "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
            "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
            "50%    0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00  0.000000e+00   \n",
            "75%    0.000000e+00  0.000000e+00  0.000000e+00  3.000000e+00  0.000000e+00   \n",
            "max    1.020000e+02  2.100000e+01  3.100000e+01  3.000000e+02  1.870000e+02   \n",
            "\n",
            "       sexual_explicit  identity_annotator_count  toxicity_annotator_count  \n",
            "count     1.804874e+06              1.804874e+06              1.804874e+06  \n",
            "mean      6.605974e-03              1.439019e+00              8.784694e+00  \n",
            "std       4.529782e-02              1.787041e+01              4.350086e+01  \n",
            "min       0.000000e+00              0.000000e+00              3.000000e+00  \n",
            "25%       0.000000e+00              0.000000e+00              4.000000e+00  \n",
            "50%       0.000000e+00              0.000000e+00              4.000000e+00  \n",
            "75%       0.000000e+00              0.000000e+00              6.000000e+00  \n",
            "max       1.000000e+00              1.866000e+03              4.936000e+03  \n",
            "\n",
            "[8 rows x 42 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJLg-yNdGB87",
        "colab_type": "code",
        "outputId": "82461097-02bc-4542-951a-4de9a26e8a75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        }
      },
      "source": [
        "trainset.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>severe_toxicity</th>\n",
              "      <th>obscene</th>\n",
              "      <th>identity_attack</th>\n",
              "      <th>insult</th>\n",
              "      <th>threat</th>\n",
              "      <th>asian</th>\n",
              "      <th>atheist</th>\n",
              "      <th>...</th>\n",
              "      <th>article_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>funny</th>\n",
              "      <th>wow</th>\n",
              "      <th>sad</th>\n",
              "      <th>likes</th>\n",
              "      <th>disagree</th>\n",
              "      <th>sexual_explicit</th>\n",
              "      <th>identity_annotator_count</th>\n",
              "      <th>toxicity_annotator_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>59848</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>2006</td>\n",
              "      <td>rejected</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>59849</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>Thank you!! This would make my life a lot less...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>2006</td>\n",
              "      <td>rejected</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>59852</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>This is such an urgent design problem; kudos t...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>2006</td>\n",
              "      <td>rejected</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>59855</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>Is this something I'll be able to install on m...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>2006</td>\n",
              "      <td>rejected</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>59856</td>\n",
              "      <td>0.893617</td>\n",
              "      <td>haha you guys are a bunch of losers.</td>\n",
              "      <td>0.021277</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.021277</td>\n",
              "      <td>0.87234</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2006</td>\n",
              "      <td>rejected</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 45 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      id    target                                       comment_text  \\\n",
              "0  59848  0.000000  This is so cool. It's like, 'would you want yo...   \n",
              "1  59849  0.000000  Thank you!! This would make my life a lot less...   \n",
              "2  59852  0.000000  This is such an urgent design problem; kudos t...   \n",
              "3  59855  0.000000  Is this something I'll be able to install on m...   \n",
              "4  59856  0.893617               haha you guys are a bunch of losers.   \n",
              "\n",
              "   severe_toxicity  obscene  identity_attack   insult  threat  asian  atheist  \\\n",
              "0         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
              "1         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
              "2         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
              "3         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
              "4         0.021277      0.0         0.021277  0.87234     0.0    0.0      0.0   \n",
              "\n",
              "   ...  article_id    rating  funny  wow  sad  likes  disagree  \\\n",
              "0  ...        2006  rejected      0    0    0      0         0   \n",
              "1  ...        2006  rejected      0    0    0      0         0   \n",
              "2  ...        2006  rejected      0    0    0      0         0   \n",
              "3  ...        2006  rejected      0    0    0      0         0   \n",
              "4  ...        2006  rejected      0    0    0      1         0   \n",
              "\n",
              "   sexual_explicit  identity_annotator_count  toxicity_annotator_count  \n",
              "0              0.0                         0                         4  \n",
              "1              0.0                         0                         4  \n",
              "2              0.0                         0                         4  \n",
              "3              0.0                         0                         4  \n",
              "4              0.0                         4                        47  \n",
              "\n",
              "[5 rows x 45 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSWSUAZVx-00",
        "colab_type": "code",
        "outputId": "69838bcb-ce73-48a4-93d1-21d8a2921bd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        }
      },
      "source": [
        "trainset.tail()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>severe_toxicity</th>\n",
              "      <th>obscene</th>\n",
              "      <th>identity_attack</th>\n",
              "      <th>insult</th>\n",
              "      <th>threat</th>\n",
              "      <th>asian</th>\n",
              "      <th>atheist</th>\n",
              "      <th>...</th>\n",
              "      <th>article_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>funny</th>\n",
              "      <th>wow</th>\n",
              "      <th>sad</th>\n",
              "      <th>likes</th>\n",
              "      <th>disagree</th>\n",
              "      <th>sexual_explicit</th>\n",
              "      <th>identity_annotator_count</th>\n",
              "      <th>toxicity_annotator_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1804869</th>\n",
              "      <td>6333967</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>Maybe the tax on \"things\" would be collected w...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>399385</td>\n",
              "      <td>approved</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1804870</th>\n",
              "      <td>6333969</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>What do you call people who STILL think the di...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>399528</td>\n",
              "      <td>approved</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1804871</th>\n",
              "      <td>6333982</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>thank you ,,,right or wrong,,, i am following ...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>399457</td>\n",
              "      <td>approved</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1804872</th>\n",
              "      <td>6334009</td>\n",
              "      <td>0.621212</td>\n",
              "      <td>Anyone who is quoted as having the following e...</td>\n",
              "      <td>0.030303</td>\n",
              "      <td>0.030303</td>\n",
              "      <td>0.045455</td>\n",
              "      <td>0.621212</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>399519</td>\n",
              "      <td>approved</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1804873</th>\n",
              "      <td>6334010</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>Students defined as EBD are legally just as di...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>399318</td>\n",
              "      <td>approved</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 45 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              id    target                                       comment_text  \\\n",
              "1804869  6333967  0.000000  Maybe the tax on \"things\" would be collected w...   \n",
              "1804870  6333969  0.000000  What do you call people who STILL think the di...   \n",
              "1804871  6333982  0.000000  thank you ,,,right or wrong,,, i am following ...   \n",
              "1804872  6334009  0.621212  Anyone who is quoted as having the following e...   \n",
              "1804873  6334010  0.000000  Students defined as EBD are legally just as di...   \n",
              "\n",
              "         severe_toxicity   obscene  identity_attack    insult  threat  asian  \\\n",
              "1804869         0.000000  0.000000         0.000000  0.000000     0.0    NaN   \n",
              "1804870         0.000000  0.000000         0.000000  0.000000     0.0    NaN   \n",
              "1804871         0.000000  0.000000         0.000000  0.000000     0.0    NaN   \n",
              "1804872         0.030303  0.030303         0.045455  0.621212     0.0    NaN   \n",
              "1804873         0.000000  0.000000         0.000000  0.000000     0.0    NaN   \n",
              "\n",
              "         atheist  ...  article_id    rating  funny  wow  sad  likes  disagree  \\\n",
              "1804869      NaN  ...      399385  approved      0    0    0      0         0   \n",
              "1804870      NaN  ...      399528  approved      0    0    0      0         0   \n",
              "1804871      NaN  ...      399457  approved      0    0    0      0         0   \n",
              "1804872      NaN  ...      399519  approved      0    0    0      0         0   \n",
              "1804873      NaN  ...      399318  approved      0    0    0      0         0   \n",
              "\n",
              "         sexual_explicit  identity_annotator_count  toxicity_annotator_count  \n",
              "1804869              0.0                         0                         4  \n",
              "1804870              0.0                         0                         4  \n",
              "1804871              0.0                         0                         4  \n",
              "1804872              0.0                         0                        66  \n",
              "1804873              0.0                         0                         4  \n",
              "\n",
              "[5 rows x 45 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f2GJt8tPXwq",
        "colab_type": "text"
      },
      "source": [
        "##Create a Corpus (vocabulary and so on) (no terminado)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5jRlInUPuN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_vocab(texts):\n",
        "    sentences = texts.apply(lambda x: x.split()).values\n",
        "    vocab = {}\n",
        "    for sentence in sentences:\n",
        "        for word in sentence:\n",
        "            try:\n",
        "                vocab[word] += 1\n",
        "            except KeyError:\n",
        "                vocab[word] = 1\n",
        "    return vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvVaaXWVP0ax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = build_vocab(df['comment_text']) #QUÉ ES QUESTION_TEXT voy a sustituir por comment_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7NnsrbQP4hr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['lowered_question'] = df['question_text'].apply(lambda x: x.lower())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gevpzt8Obvk",
        "colab_type": "text"
      },
      "source": [
        "### 03 - Cleaning the Dataset\n",
        "Dropping columns we do not need for the type of training we are going to choose, eliminating NaNs, normalizing variables, removing outliers (scores too high, texts too long, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bRFBzRKbE-UW",
        "colab": {}
      },
      "source": [
        "# Drops non relevant columns: Creates a list of labels, then passes that list \n",
        "# to .drop() for the columns with those labels to be dropped from the dataframe\n",
        "droplist = ['created_date','publication_id', 'parent_id', 'article_id', \n",
        "            'rating', 'funny', 'wow', 'sad', 'likes', 'disagree', \n",
        "            'identity_annotator_count', 'toxicity_annotator_count']\n",
        "trainset_clean = trainset.drop(columns = droplist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNbtidbaFFCU",
        "colab_type": "code",
        "outputId": "880e1b19-44d4-4a30-e98c-90016c907acf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "print(trainset_clean.shape)\n",
        "trainset_clean.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1804874, 33)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'target', 'comment_text', 'severe_toxicity', 'obscene',\n",
              "       'identity_attack', 'insult', 'threat', 'asian', 'atheist', 'bisexual',\n",
              "       'black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu',\n",
              "       'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability',\n",
              "       'jewish', 'latino', 'male', 'muslim', 'other_disability',\n",
              "       'other_gender', 'other_race_or_ethnicity', 'other_religion',\n",
              "       'other_sexual_orientation', 'physical_disability',\n",
              "       'psychiatric_or_mental_illness', 'transgender', 'white',\n",
              "       'sexual_explicit'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7u0ZKRbYuiQ",
        "colab_type": "code",
        "outputId": "7772e9d6-df52-4b6f-9f86-7637beabc327",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "# Checks number of NaN values per column.\n",
        "trainset_clean.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id                                           0\n",
              "target                                       0\n",
              "comment_text                                 0\n",
              "severe_toxicity                              0\n",
              "obscene                                      0\n",
              "identity_attack                              0\n",
              "insult                                       0\n",
              "threat                                       0\n",
              "asian                                  1399744\n",
              "atheist                                1399744\n",
              "bisexual                               1399744\n",
              "black                                  1399744\n",
              "buddhist                               1399744\n",
              "christian                              1399744\n",
              "female                                 1399744\n",
              "heterosexual                           1399744\n",
              "hindu                                  1399744\n",
              "homosexual_gay_or_lesbian              1399744\n",
              "intellectual_or_learning_disability    1399744\n",
              "jewish                                 1399744\n",
              "latino                                 1399744\n",
              "male                                   1399744\n",
              "muslim                                 1399744\n",
              "other_disability                       1399744\n",
              "other_gender                           1399744\n",
              "other_race_or_ethnicity                1399744\n",
              "other_religion                         1399744\n",
              "other_sexual_orientation               1399744\n",
              "physical_disability                    1399744\n",
              "psychiatric_or_mental_illness          1399744\n",
              "transgender                            1399744\n",
              "white                                  1399744\n",
              "sexual_explicit                              0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IniY6ytONYgF",
        "colab_type": "text"
      },
      "source": [
        "####Cleaning contractions (no terminado)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQhipUCqOGsl",
        "colab_type": "text"
      },
      "source": [
        "Sustituimos contracciones por sus expresiones explícitas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyXfJwA9N3F9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaAHLEoKOXf0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_contractions(text, mapping):\n",
        "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
        "    for s in specials:\n",
        "        text = text.replace(s, \"'\")\n",
        "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNvQ48WiOmtD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#falla esto TODO\n",
        "\n",
        "df['treated_question'] = df['lowered_question'].apply(lambda x: clean_contractions(x, contraction_mapping))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqjLJsYJNsrN",
        "colab_type": "text"
      },
      "source": [
        "####Cleaning punctuation (no terminado)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqMLSGH9OxfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO\n",
        "def preprocess(data):\n",
        "    '''\n",
        "    Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n",
        "    '''\n",
        "    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
        "    def clean_special_chars(text, punct):\n",
        "        for p in punct:\n",
        "            text = text.replace(p, ' ')\n",
        "        return text\n",
        "\n",
        "    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mvaxscRO1ds",
        "colab_type": "text"
      },
      "source": [
        "### 04 - Tokenization & Defining Dataloaders\n",
        "Splitting the dataset into training and validation (test set already available), defining transforms, creating dataloaders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIoMqSLQLpbC",
        "colab_type": "code",
        "outputId": "0b5e621d-1617-4822-d0c5-b176b67ea24e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from flair.data import Sentence\n",
        "TrySentence = Sentence(trainset_clean.iloc[0,2])\n",
        "print(TrySentence)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence: \"This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\" - 19 Tokens\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX0s-OHYQl0m",
        "colab_type": "code",
        "outputId": "f44df52a-6ab9-4545-a4c0-0c961182bfa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "for token in TrySentence:\n",
        "  print(token)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token: 1 This\n",
            "Token: 2 is\n",
            "Token: 3 so\n",
            "Token: 4 cool.\n",
            "Token: 5 It's\n",
            "Token: 6 like,\n",
            "Token: 7 'would\n",
            "Token: 8 you\n",
            "Token: 9 want\n",
            "Token: 10 your\n",
            "Token: 11 mother\n",
            "Token: 12 to\n",
            "Token: 13 read\n",
            "Token: 14 this??'\n",
            "Token: 15 Really\n",
            "Token: 16 great\n",
            "Token: 17 idea,\n",
            "Token: 18 well\n",
            "Token: 19 done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ2bo8YIcDCO",
        "colab_type": "text"
      },
      "source": [
        "####Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAG85omzjwxo",
        "colab_type": "text"
      },
      "source": [
        "Convertimos cada palabra en un tensor o vector con el que operar desde PyTorch. Usaremos el embedding GloVe en el constructor WordEmbeddings('string').\n",
        "\n",
        "Link desde Recommended Flair Usage: https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_4_ELMO_BERT_FLAIR_EMBEDDING.md"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbV_zSUijzn2",
        "colab_type": "code",
        "outputId": "adbcdefd-dca6-4148-d315-b6a64530bda2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# TODO: Use non-deprecated function\n",
        "from flair.embeddings import WordEmbeddings, FlairEmbeddings, StackedEmbeddings\n",
        "\n",
        "# create a StackedEmbedding object that combines glove and forward/backward flair embeddings\n",
        "stacked_embeddings = StackedEmbeddings([\n",
        "                                        WordEmbeddings('glove'), \n",
        "                                        FlairEmbeddings('news-forward'), \n",
        "                                        FlairEmbeddings('news-backward'),\n",
        "                                       ])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-04 17:23:02,215 this function is deprecated, use smart_open.open instead\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C73WV1VSj2Wh",
        "colab_type": "code",
        "outputId": "c980485e-ebca-452e-f3ff-fefa6dda8baa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "\n",
        "sentence = Sentence('The grass is green .')\n",
        "\n",
        "# just embed a sentence using the StackedEmbedding as you would with any single embedding.\n",
        "stacked_embeddings.embed(sentence)\n",
        "\n",
        "# now check out the embedded tokens.\n",
        "for token in sentence:\n",
        "    print(token)\n",
        "    print(token.embedding)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token: 1 The\n",
            "tensor([-3.8194e-02, -2.4487e-01,  7.2812e-01,  ..., -4.4014e-04,\n",
            "        -3.9301e-02,  1.0601e-02])\n",
            "Token: 2 grass\n",
            "tensor([-8.1353e-01,  9.4042e-01, -2.4048e-01,  ..., -3.7749e-04,\n",
            "        -2.3563e-02,  1.1700e-02])\n",
            "Token: 3 is\n",
            "tensor([-0.5426,  0.4148,  1.0322,  ..., -0.0061,  0.0112,  0.0100])\n",
            "Token: 4 green\n",
            "tensor([-0.6791,  0.3491, -0.2398,  ..., -0.0026, -0.0118,  0.0455])\n",
            "Token: 5 .\n",
            "tensor([-3.3979e-01,  2.0941e-01,  4.6348e-01,  ..., -2.3405e-04,\n",
            "         3.8688e-03,  5.7725e-03])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf6XCPXNJLn7",
        "colab_type": "text"
      },
      "source": [
        "##Positive or negative sentence?\n",
        "TextClassifier from Flair says if a sentence has a positive mood or a negative one.\n",
        "\n",
        "Obligatorio: usar las dos comillas al principio y al final. No incluir ninguna otra en medio.\n",
        "\n",
        "Apreciaciones: ha tardado casi 2 minutos en devolvernos un resultado. ¿Servirá para TODO el dataset? No, al menos hasta donde he llegado.\n",
        "\n",
        "Link: https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_2_TAGGING.md\n",
        "\n",
        "Sección: Tagging with Pre-Trained Text Classification Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OALOkm86Ofxh",
        "colab_type": "text"
      },
      "source": [
        "Añadiendo una frase directamente del dataset peta en Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOwNpo_hJbBh",
        "colab_type": "code",
        "outputId": "b46d910a-fe56-40c6-88c6-6ec3de215a2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from flair.models import TextClassifier\n",
        "\n",
        "from flair.data import Sentence #esto se usó antes en Tokenization & Defining Dataloaders\n",
        "\n",
        "classifier = TextClassifier.load('en-sentiment') \n",
        "\n",
        "# predict NER tags\n",
        "classifier.predict(TrySentence)\n",
        "\n",
        "# print sentence with predicted labels\n",
        "print(TrySentence.labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-04 00:21:53,110 loading file /root/.flair/models/imdb.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqAEwSorYu8C",
        "colab_type": "text"
      },
      "source": [
        "Probamos con comillas dobles al principio y al final de la frase. It works!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMPZKPE-Yr8r",
        "colab_type": "code",
        "outputId": "aa841fdd-6482-45a1-bb06-403ad571ae30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "from flair.models import TextClassifier\n",
        "\n",
        "classifier = TextClassifier.load('en-sentiment')\n",
        "\n",
        "sentence = Sentence(\"This is so cool. Well done!\")\n",
        "\n",
        "# predict NER tags\n",
        "classifier.predict(sentence)\n",
        "\n",
        "# print sentence with predicted labels\n",
        "print(sentence.labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-04 15:08:24,916 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models-v0.4/TEXT-CLASSIFICATION_imdb/imdb.pt not found in cache, downloading to /tmp/tmpmz27jh4f\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2794252905/2794252905 [00:45<00:00, 60858183.03B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-05-04 15:09:10,990 copying /tmp/tmpmz27jh4f to cache at /root/.flair/models/imdb.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-05-04 15:09:29,538 removing temp file /tmp/tmpmz27jh4f\n",
            "2019-05-04 15:09:29,541 loading file /root/.flair/models/imdb.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:574: DeprecationWarning: Call to deprecated class DocumentLSTMEmbeddings. (The functionality of this class is moved to 'DocumentRNNEmbeddings') -- Deprecated since version 0.4.\n",
            "  result = unpickler.load()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[POSITIVE (1.0)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xls2W-ZpPVgH",
        "colab_type": "text"
      },
      "source": [
        "###05 - Defining Network Architecture\n",
        "Defining our LSTM, creating and implementing its functions (train, backprop, etc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xu1pZ8fJZcY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpfIFfgzPvkr",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jI2u4v2PyHS",
        "colab_type": "text"
      },
      "source": [
        "###06 - Training the Model\n",
        "Defining the model´s hyperparameters.\n",
        "Running our train data through the model, checking loss, saving the better fitting model, fine tuning hyperparameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7F18Mu9RyrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
        "test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n",
        "\n",
        "x_train = train['comment_text']\n",
        "y_train = np.where(train['target'] >= 0.5, 1, 0)\n",
        "y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n",
        "x_test = test['comment_text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0clnPwVOQCC7",
        "colab_type": "text"
      },
      "source": [
        "###07 - Validating the Model\n",
        "Running our validation dataset through the model. Repeating step 06 if necessary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvvFR5ZrQtMJ",
        "colab_type": "text"
      },
      "source": [
        "###08 - Testing the model\n",
        "After 06 and 07 have been completed satisfactorily, running the test set through the model. If the model generalizes well, then saving the model. Otherwise, start from 6 from scratch."
      ]
    }
  ]
}